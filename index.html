<!DOCTYPE html>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.3/css/lightbox.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.3/js/lightbox.min.js"></script>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Hunting for Insights: Investigating Predator-Prey Dynamics through
        Simulated Vision and Reinforcement Learning.">
  <meta name="keywords" content="Predator-Prey, Vision, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Hunting for Insights: Investigating Predator-Prey Dynamics through
    Simulated Vision and Reinforcement Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script-->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/Tiger.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://vilab.epfl.ch/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Hunting for Insights: Investigating Predator-Prey Dynamics through
            Simulated Vision and Reinforcement Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://arvind6599.github.io/">Arvind Menon</a>,</span>
            <span class="author-block">
              <a href="https://lars-quaedvlieg.github.io/">Lars C.P.M. Quaedvlieg</a>,</span>
            <span class="author-block">
              <a href="https://vilab.epfl.ch/#prospective">Somesh Mehra</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Swiss Federal Institute of Technology Lausanne (EPFL)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lars-quaedvlieg/Predator-Prey-Unity-MLAgents"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/K0tJrpMla-o"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/arvind6599/PredatorPreyWebsite"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Website</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="box">
      <img src="https://imgtr.ee/images/2023/06/01/S2FLI.png">
      <!--video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%" width="100%" poster="static/videos/loading.gif">
        <source src="static/videos/palmer_trimmed.mp4" type="video/mp4">
      </video-->
    </div>
    <br>
  </div>
</section>

<!-- USEFUL SECTION - static video with a caption -->
<!--section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section-->


<!-- USEFUL SECTION - carousel of videos, can add captions too -->
<!-- section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
            free-viewpoint
            portraits.
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section -->




<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This study investigates how different vision fields affect
            predator-prey interactions. By simulating simplified environments
            and training agents with reinforcement learning, we
            observe trends that emerge in the strategies and effectiveness
            of trained predator and prey agents which use varying vision
            fields. Our findings support our current understanding that
            depth perception is very important for predators, whilst field
            of view is crucial for prey. Our work adds to existing
            literature regarding predator-prey simulations. By implementing
             some modifications to the setup, our approach enables
            exploring the complex interactions between predators and prey
            in new ways.
          </p>
        </div>

      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">

        <h2 class="title is-3">Overview</h2>
        <div class="tile is-ancestor is-centered" style="vertical-align: middle;">
          <div class="tile is-parent">
            <a href="#intro" class="tile is-child box">
              <p class="subtitle"><font size="-1">Introduction</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#related-work" class="tile is-child box">
              <p class="subtitle"><font size="-1">Related Work</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#method" class="tile is-child box">
              <p class="subtitle"><font size="-1">Method</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#exp" class="tile is-child box">
              <p class="subtitle"><font size="-1">Experiments</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#conclusion" class="tile is-child box">
              <p class="subtitle"><font size="-1">Conclusion and Limitations</font></p>
            </a>
          </div>
        </div>

      </div>
    </div>
    <!--/ Abstract. -->


    <!-- USEFUL SECTION - embedded youtube video. -->
    <!--div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div-->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="intro">I. Introduction</h2>
        <div class="content has-text-justified">
          <p>
            The natural world is full of fascinating and complex interactions between predators and prey, with each constantly
            adapting and evolving to survive. As researchers seek to better understand these dynamics, visual intelligence has
            emerged as a critical field of study, allowing us to gain new insights into how animals perceive and react to their
            environments.
          </p>
          <p>
            In this work, we investigate the role of vision in prey-predator settings by leveraging reinforcement learning
            to train agents in simulated environments. Specifically, we are interested in how the field of view and the region of
            binocular vision affect the strategies and effectiveness used by predators and prey to hunt or evade respectively.
            We train predator and prey agents with varying vision fields in non-trivial environments with obstacles. The
            vision fields we experiment with are inspired by typical real-world predators and prey.
          </p>
          <p>
            To evaluate the effectiveness of our approach, we both quantitatively evaluate agent performance in new environments and qualitatively observe
            the emergent strategies and behaviours of the agents under different configurations. Our work builds upon existing literature in this area to gain a deeper understanding of how differences in visual
            perception can influence predator-prey interactions.
          </p>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Related work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="related-work">II. Related work</h2>
        <div class="content has-text-justified">
          <p>
            The exploration of predator-prey dynamics in the context of vision has received relatively little attention
            in the existing literature. Research on simulated predator-prey systems commonly focuses on population dynamics
            and deals with very simple environments. At the same time, there have been major advancements in multi-agent
            reinforcement learning (MARL), making it possible to discover interesting group dynamics in complex environments.
            Given this, we present below some prior work related to this project.
          </p>
        </div>
      </div>
    </div>
    <!--/ Related work. -->
  </div>
  <br>

  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <!--Predator prey Ecosystems. -->

      <div class="column">
        <div class="content">
          <h2 class="title is-6">Co-Evolution of Predator-Prey Ecosystems by Reinforcement Learning Agents
                                 <a href="https://doi.org/10.3390/e23040461">[1]</a></h2>
          <p>
            This paper explores the use of MARL techniques to simulate the co-evolution mechanisms in
            predator-prey ecosystems. The study demonstrates a biologically plausible approximation of
            the agents' co-evolution over multiple generations in nature.
          </p>
          <div class="box">
            <img src="static/images/predator-prey-behaviour-related-work.png"
                 alt="Comparison of potential distance metric that correlate with reachability.">
            <h5 class="subtitle has-text-centered">
         			<font size="-1.5">
                Left: <span style="font-weight:normal">Initial random location of predators and prey.</span> <br>
                Right: <span style="font-weight:normal">Emergence of swarming among predators and prey.</span>
         			</font>
       			</h5>
          </div>
          <p>
            However, this work is limited to a simplified 2D environment without incorporating vision.
          </p>
        </div>
      </div>
      <!--/ Predator prey Ecosystems.-->

      <!-- OpenAI hide and seek. -->
      <div class="column">
        <h2 class="title is-6">Emergent tool use from multi-agent autocurricula
                               <a href="https://openai.com/research/emergent-tool-use">[2]</a></h2>
        <p>
          Research conducted by OpenAI investigates the emergence of sophisticated tool use and coordination among agents in
          a hide-and-seek game environment, which shares similarities with predator-prey interactions. The study showcases the
          potential of multi-agent self-play in generating emergent auto curricula. Notably, this work trains agents in a
          complex environment equipped with vision sensors.

        </p>
        <div class="box">
          <img src="static/images/openai-hide-and-seek.gif"
               alt="OpenAI Hide and Seek">
          <h5 class="subtitle has-text-centered">
            <font size="-1.5">
              <span style="font-weight:normal">Use of vision sensors in the hide-and-seek environment</span>
            </font>
          </h5>
        </div>
         <p>
           However, this study does not explicitly explore the effects of varying the vision fields on the emergent behaviours of the agents.
         </p>
      </div>
    </div>
      <!--/ OpenAI hide and seek. -->
  </div>
  <br>

  <div class="container is-max-desktop">
    <!-- Related work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Given the existing gaps in the literature for the study of predator-prey dynamics in the context of vision, our work aims
            to address them by incorporating various vision fields to represent different predators and prey. Through this approach,
            we investigate the impact of diverse visual perspectives on the emergent behaviours of the agents, providing valuable insights
            into the adaptive strategies employed in predator-prey interactions.
          </p>
        </div>
      </div>
    </div>
    <!--/ Related work. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="method">III. Method</h2>
        <div class="content has-text-justified">
          <p>
            In this section, we present the methodology utilized for this paper. We first discuss the environment setup,
            after which we detail how the predator and prey agents are created. Finally, we talk about how the agents are optimized.
          </p>
          <p>
            We create a 3D environment using the Unity game development engine <a href="https://unity.com/">[3]</a>. The selection of Unity as the engine
            for creating the environment instead of a different simulator is motivated by several key factors. Mainly, Unity's ML-Agents
            package <a href="https://arxiv.org/abs/1809.02627">[4]</a> offers support for the OpenAI Gym framework
            <a href="https://arxiv.org/abs/1809.02627">[5]</a>, which is widely used in
            reinforcement learning research. This way, Unity enables interaction between the environment and the learning algorithms,
            allowing for efficient experimentation and evaluation of predator-prey agents.
          </p>
          <p>
            Moreover, the engine offers a wide range of built-in tools, such as physics simulations, ray tracing, and collision.
            Finally, Unity provides a user-friendly and intuitive development environment, making it accessible to researchers with
             varying levels of expertise.
          </p>
        </div>
        <!-- Creating environments. -->
        <h3 class="title is-4">III-A. Environments</h3>
        <div class="content has-text-justified">
          <p>
            We present a diverse set of simulated environments aimed at replicating various scenarios involving predator-prey interactions.
            Our objective is to uncover a wide range of behaviors for the predators and prey. To achieve
            this diversity, we incorporate obstacles such as trees and rocks within the environments. Additionally, we introduce walls along
            the boundaries to confine the agents to the designated training area. This confinement allows us to concentrate the actions of an agent
             and observations within a controlled environment.
          </p>
          <p>
            Furthermore, we integrate physics-based simulations and collision detection mechanisms into the environment.
            By assigning colliders to the predator and prey agent models, we ensure accurate detection of interactions with the
            environment and other agents. Each object in the environment is assigned a corresponding tag, such as "obstacle",
            "predator", or "prey", enabling efficient identification.
          </p>
          <p>
            To speed up the learning process, we design the initial training environment ('Control' shown below) to be free of
            obstacles. This approach eliminates potential obstructions that could slow down training progress in the early stages.
            Subsequently, we create three additional training environments ('Forest', 'Escape Room' and 'Rocky') featuring distinct obstacle types:
            small trees, large rocks, and a split doorway. These environments allow the agents to acquire the skills necessary for
            navigating through increasingly complex situations involving obstacles, and should allow the prey agent to learn to exploit
             occlusions.
          </p>
        </div>
        <!--/ Creating environments. -->

      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <a href="static/images/training/control_train.jpg" data-lightbox="image">
            <img src="static/images/training/control_train.jpg" alt="Image 1">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Control</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment 1</p>
          </div>
        </div>
        <div class="item">
          <a href="static/images/training/forrest.jpg" data-lightbox="image">
            <img src="static/images/training/forrest.jpg" alt="Image 2">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Forest</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment 2</p>
          </div>
        </div>
        <div class="item">
          <a href="static/images/training/holes.jpg" data-lightbox="image">
            <img src="static/images/training/holes.jpg" alt="Image 3">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Escape Room</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment 3</p>
          </div>
        </div>
        <div class="item">
          <a href="static/images/training/rocky.jpg" data-lightbox="image">
            <img src="static/images/training/rocky.jpg" alt="Image 4">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Rocky</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment 4</p>
          </div>
        </div>
      </div>
    </div>
    <h6 class="subtitle has-text-centered">
      Environments used for training the agents
    </h6>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- Creating agents. -->
        <h3 class="title is-4">III-B. Predator and Prey Agents</h3>
        <div class="content has-text-justified">
          <p>
            Whilst the prey and predator have opposing objectives, they share similar characteristics. Both agents operate
            within a continuous action space with two degrees of freedom: rotation about the \(y\)-axis and forward/backward movement.
            Each degree of freedom allows the agent to select a value between \([-1, 1]\), determining the direction and intensity of
             movement. The agents' behavioural parameters consist of maximum movement and rotation speeds, which scale this intensity
              value. Notably, the predator possesses an additional box collider located at its face, resulting in the prey's death
              upon collision. Conversely, the prey lacks this collider as its primary goal involves evading the predator.
          </p>
          <br>
          <div class="columns is-centered">
            <div class="column is-three-fifths">
              <div align="center" class="box">
                <img src="static/images/agent-box-colliders.jpeg" alt="Box colliders for the agents">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Box colliders for the predator and prey agents</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

          <br>
        </div>
        <!--/ Creating agents. -->

        <!-- Vision sensor. -->
        <h3 class="title is-5">Vision sensor</h3>
        <div class="content has-text-justified">
          <p>
            In order to simulate various vision fields, we employ a modified version of the ML-Agents Ray Perception sensor. This
            sensor uses a collection of ray casts originating from a central point. Several parameters are available to manipulate
             the vision, including ray angles (to establish the field of view), ray length (to determine the depth of field), number
             of rays (to regulate ray density/resolution), and a list of tags for filtering detected objects. Each ray conveys the following
             information: a flag indicating whether it collided with a tagged object, a one-hot encoded vector identifying the object
              type, and the normalized distance to the hit object relative to the ray length. Consequently, the sensor's output
              comprises a flattened vector encapsulating the information encoded within each ray.
          </p>
          <p>
            To control the region of binocular vision, we have extended this sensor to incorporate a parameter representing
            the number of depth rays. We make the simple assumption that agents can see the presence and type
            of an object across their entire field of view, but only have depth perception in their binocular region.
            Consequently, we exclude depth information for any rays falling outside this region by
            assigning a distance of \(-1\). Using this modified Ray Perception sensor, we define two distinct sensor types:
            <em>predator-style</em> and <em>prey-style</em>, which draw inspiration from the characteristic vision traits observed in real-world
            predators and prey <a href="https://books.google.ch/books?id=uXSK6hDKFC0C&oi=fnd&pg=PP1&dq=animal+eyes+&ots=dLMn7ufvNI&sig=Aedtf7xH87s3c9gMFKzPWSvj1Ms&redir_esc=y#v=onepage&q=animal%20eyes&f=false">[16]</a>, respectively.
          </p>
          <br>
          <div align="center" class="box">
            <img src="static/images/ray-perception-sensors.png" alt="Predator and prey-style ray perception sensors">
            <h5 class="subtitle has-text-centered">
              <font size="-0.7">
                <span style="font-weight:normal">Predator- and prey-style ray perception sensors</span>
              </font>
            </h5>
          </div>

          <br>
          <p>
            Utilizing the modified Ray Perception sensor offers distinct advantages for this paper by providing precise
             control over the simulated vision fields. Customizable parameters such as field of view and depth of field enable
             alignment with the specific requirements of the prey and predator agents in the simulated environment. This level of
             control ensures that the sensory input accurately reflects the desired characteristics of the vision of the agent.
             Furthermore, using the modified Ray Perception sensor instead of raw camera inputs enhances training efficiency by
             reducing the computational burden of processing high-dimensional image data.
          </p>
          <br>
        </div>
        <!--/ Vision sensor. -->

        <!-- Training. -->
        <h3 class="title is-4">III-C. Setting up Reinforcement Learning Training</h3>
        <div class="content has-text-justified">
          <p>
            The setting of predator-prey dynamics can be framed as a two-player dynamic game <a href="https://ieeexplore.ieee.org/abstract/document/7868186">[6]</a>,
            <a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611971132.bm">[7]</a>. This formulation captures the interactions and influence of the agents on the state of the game, providing a framework
            for studying predator-prey dynamics based on a common utility function. The game is only played for a set number of
            steps, until it is terminated.
          </p>

          <p>
            In our formulation, the state and action spaces align with the descriptions provided in the previous sections,
            allowing for a consistent representation of the game dynamics. The transitions between states are determined by
            the underlying physics engine, ensuring a realistic simulation of the predator-prey environment.
          </p>

          <p>
            $$
              G_\text{pred} = \sum_{i=1}^{T}\left[\frac{𝟙(\text{prey caught at } t)}{N_\text{prey}} - \frac{1}{T}\right]
            $$
            $$
              G_\text{prey} = - G_\text{pred}
            $$
          </p>

          <p>
            The reward structure employed in our proposed framework, as illustrated in the figure above, incorporates a constant time
            penalty applied to the predator at each time step, and a positive reward if a prey is caught. This deliberate design
            choice serves to incentivize the predator to capture prey at the earliest opportunity. Conversely, the reward for the
            prey is formulated in the opposite manner, establishing a zero-sum game. The range of rewards is bounded within the
            interval \([-1, 1]\). This reward specification enables a natural distinction between favorable and unfavorable
            outcomes of both agents.
          </p>

          <p>
            Furthermore, since the predator-prey game exhibits an inherent asymmetry, it can be expressed as an asymmetric
            zero-sum game. Unlike games such as soccer, where teams have shared objectives, the predator and prey agents
            pursue conflicting goals. The asymmetry arises from the distinct policies used by the different agents, resulting
            in strategic dynamics that differ from those observed in symmetric games.
          </p>

          <p>
            Overall, the nature of the game poses additional challenges, as it suffers from many of the problems in
            competitive multi-agent learning <a href="https://link.springer.com/chapter/10.1007/978-3-642-14435-6_7">[8]</a>. Multi-agent reinforcement learning in competitive scenarios
            poses several challenges <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=46662ca0627974ec3c3c038ccc78b2ef9ef92218">[9]</a>. In this paper, we employ a prominent technique called "self-play"
            <a href="https://proceedings.neurips.cc/paper/2020/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf">[10]</a>, <a href="https://www.science.org/doi/pdf/10.1126/science.aar6404?casa_token=U6oPNZ67XRcAAAAA%3Ai-nCPopyruTropl7_QoP3RWxM93tlHd75lV826n7-8X5_XlX0EniJPBxQ9KnsNYUHSxFyeLGBa8juko&">[11]</a>
            to address these difficulties. The diagram below provides a high-level overview of this approach.
          </p>
          <br>
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <div align="center" class="box">
                <img src="static/images/self-play.jpeg" alt="Self play">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Diagram showing the self-play mechanism</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>



          <br>

          <p>
            Initially, either the predator or prey agent is trained, while the model of the other agent remains frozen. After
            a predetermined number of iterations, the frozen model is trained, and the other agent's model is frozen.
            However, a potential issue arises from the bias introduced by repeatedly playing against the most recent model.
            This bias can lead to overfitting and poor generalization. To mitigate this problem, we incorporate an ELO ranking
            system as done in MuZero <a href="https://www.nature.com/articles/s41586-020-03051-4">[12]</a>, enabling models to compete against earlier versions of their opponents.
            By doing so, we reduce the impact of bias, leading to improved generalization and robustness in the learned policies.
          </p>
          <br>

        </div>
        <!--/ Training. -->

      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="exp">IV. Experiments</h2>

        <p>
          In this section, we discuss the setup and results of the experiments in detail. Regarding results, we first discuss the training progression, after which we do both a qualitative and
          a quantitative analysis of the different agent configurations.
        </p>
        <br>

        <!-- Inference environments. -->
        <h3 class="title is-4">IV-A. Experimental Setup</h3>

        <br/>
        <div class="content has-text-justified">
          <div class="container">
            <!-- <h2 class="title is-2 has-text-centered">Model and Behavior Parameters</h2> -->
                <h3 class="title is-5">Agent parameters</h3>

                <div class="box">
                  <div class="table-container">
                      <table class="table" align="center">
                          <thead>
                              <tr>
                                  <th></th>
                                  <th>Rays per direction</th>
                                  <th>Depth rays per direction</th>
                                  <th>Max ray degrees</th>
                                  <th>Ray length</th>
                                  <th>Observation stacks</th>
                                  <th>Field of view</th>
                                  <th>Binocular region</th>
                                  <th>Maximum movement speed</th>
                                  <th>Maximum rotation speed</th>
                              </tr>
                          </thead>
                          <tbody>
                              <tr>
                                  <td><b>Prey-style</b></td>
                                  <td>30</td>
                                  <td>4</td>
                                  <td>160</td>
                                  <td>15</td>
                                  <td>5</td>
                                  <td><em>320°</em></td>
                                  <td><em>43°</em></td>
                                  <td><em>8</em></td>
                                  <td><em>8</em></td>
                              </tr>
                              <tr>
                                  <td><b>Predator-style</b></td>
                                  <td>30</td>
                                  <td>20</td>
                                  <td>85</td>
                                  <td>15</td>
                                  <td>5</td>
                                  <td><em>170°</em></td>
                                  <td><em>113°</em></td>
                                  <td><em>6</em></td>
                                  <td><em>2</em></td>
                              </tr>
                          </tbody>
                      </table>
                  </div>
                  <div align="center">
                  <font size="-0.7">
                    <span style="font-weight:normal">
                      Parameters of the prey and predator-style vision sensors, and the behavioural parameters used for the agents.
                  </span>
                  </font>
                  </div>
                </div>

                <p>Firstly, we establish two distinct categories of vision: prey-style and predator-style.
                  This can be seen in the table above.
                  Prey-style vision emphasizes a broad field of view while limiting the binocular region,
                  whereas predator-style vision exhibits the opposite characteristics. Additionally, for both agents we
                  stack the observations to simulate a brief memory of recent events.
                </p>

                <p>
                  During inference, we designate the maximum speed of the prey to be greater
                  than that of the predator. We adopt this approach to give the prey a better chance to play out
                  its learned strategies rather than getting caught too quickly in each episode.

                </p>

                <div class="columns is-centered">
                  <div class="column is-three-fifths">
                    <div class="box">
                      <img src="static/images/sensor_configs.png" alt="Vision sensor configurations">
                      <h5 class="subtitle has-text-centered">
                        <font size="-0.7">
                          <span style="font-weight:normal">
                            The combinations of vision sensors used for the predator and prey in the experiments.
                        </span>
                        </font>
                      </h5>
                    </div>
                  </div>
                </div>

                <p>
                  As depicted in the figure above, we train agents in 4 configurations to cover each combination of
                  vision types for the predator and prey. The trained models are then used to perform inferences.
                </p>

          </div>
        </div>

        <div class="content has-text-justified">
          <div class="container">

            <h3 class="title is-5">Training details</h3>
            <p>
              The agents are trained on all environments mentioned in the methodology section in parallel, using the parameters mentioned in the agent parameters section.
              The only difference is that the maximum speed of the prey was set to \(5.5\). By having environments of different difficulties,
              a speed-up in training can be observed, similar as in curriculum learning <a href="https://dl.acm.org/doi/abs/10.1145/1553374.1553380?casa_token=nh2ZrojaWoAAAAAA:ch-UtBUm-jXdt36EJoGrONsdXTfKYTOF9g3wpMqUjykIzeeCv2_d4p2hoGlyuJRGTlWCdZyWmPVmmg">[13]</a>,
              <a href="https://dl.acm.org/doi/abs/10.5555/3455716.3455897">[14]</a>. Furthermore, three agents of each type were present
              in each environment. Although this could make the environment unstable due to partial observability, it greatly enhances training speed.
            </p>
            <p>
              The predator and prey were each trained self-play for \(9 \cdot 10^6\) steps, where one step corresponds to
              one frame in the simulation. The swap between the frozen training teams happens every \(3 \cdot 10^5\) training steps. At
              each episode, the agent is spawned with a random rotation the y-axis, and the maximum episode duration is \(10^4\).
              The model plays against the most recent version of the agent \(80\%\) of the time. The models both consist of
              a \(3\) hidden-layer MLP with \(512\) neurons with batch normalisation at each layer. The model is optimized using
              PPO with the Adam <a href="https://arxiv.org/abs/1412.6980">[15]</a> optimizer with a learning rate of \(0.0003\) and no decay. Finally, the buffer sizes of both
              agents are set to \(40960\).
            </p>

            <h3 class="title is-5">Inference environments</h3>
            <p>
              For testing the agents we design new out of distribution environments with varying levels of complexity. The control
              environment, like in training, consists of an empty environment. Additional environments include similar obstacles and
              hiding spots in different configurations, introducing more blind spots for the prey to utilize for protection. The inference
              environments are shown below.
            </p>

          </div>
        </div>

        <br/>


      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <a href="./static/images/inference/control.jpg" data-lightbox="image">
            <img src="./static/images/inference/control.jpg" alt="Control Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Control</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Control</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/trees.jpg" data-lightbox="image">
            <img src="./static/images/inference/trees.jpg" alt="Trees Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Trees</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Trees</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/rocks.jpg" data-lightbox="image">
            <img src="./static/images/inference/rocks.jpg" alt="Rocks Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Rocks</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Rocks</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/hide.jpg" data-lightbox="image">
            <img src="./static/images/inference/hide.jpg" alt="Hide Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Hide</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Hide</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/seek.jpg" data-lightbox="image">
            <img src="./static/images/inference/seek.jpg" alt="Seek Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Seek</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Seek</p>
          </div>
        </div>
      </div>
    </div>
    <h6 class="subtitle has-text-centered">
      Environments used for inference
    </h6>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <div class="content has-text-justified">
          <div class="container">
            <h3 class="title is-5">Hardware</h3>
            <p>
              The models were trained using a CUDA-enabled NVIDIA
              V100 PCIe 32 GB GPU with 7TFLOPS, a Xeon-Gold processor running with a 2.1 GHz clock speed, and 16GB of
              RAM. Our method is implemented with Unity’s ML-Agents package and PyTorch, running on Linux with Python 3.7.
            </p>

          </div>
        </div>

      </div>
    </div>
    <!--/ Experiments. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Results. -->
        <!-- Include median_time_survived.html using an iframe -->
        <h3 class="title is-4">IV-B. Results</h3>
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <div class="container">
              <!-- <h2 class="title is-2 has-text-centered">Model and Behavior Parameters</h2> -->
                  <h3 class="title is-5">Training results</h3>
                  <div class="box">
                      <img src="./static/images/training-plot.png" alt="Reward plot during training">
                      <h5 class="subtitle has-text-centered">
                        <font size="-0.7">
                          <span style="font-weight:normal">
                            (Left) Average returns over \(9\) million steps of training for each agent configuration.
                            (Right) Average episode lengths over \(9\) million steps of training for each agent configuration.
                            Note that, unfortunately, the training logs for 'Normal' vision were lost. However the trend in rewards
                            was similar to the other plots shown here. Furthermore, take note that the x-axis represents the number of steps,
                            so if the predator was trained first, it would swap at the step \(300000\), and the x-label for the prey at total step \(300005\)
                            would be \(5\). An exponential smoothing of strength \(0.8\) is used.
                        </span>
                        </font>
                      </h5>
                  </div>
                  <p>
                    The average returns during training for various vision configurations of predator and prey agents are illustrated in the left figure. It is
                    observed that the PredatorBoth vision improves at a slower rate compared to other configurations, likely due to the reduced field of
                    view when both agents have predator vision. However, this effect diminishes over time.
                  </p>
                  <p>
                    In the right figure, we analyze the episode length throughout the training process. Interestingly, we consistently observe a decreasing trend in episode
                    length across all configurations. It is important to note however that whilst it may seem like the predator is learning to consistenly dominate the
                    prey, the training environment is heavily biased towards the predator since there are three predators which can all catch the prey. Thus, to judge
                    the true performance of the agents we perform inference in a more balanced environment.
                    </thead>
                  </p>
                  <br>
            </div>


            <div class="container">
              <!-- <h2 class="title is-2 has-text-centered">Model and Behavior Parameters</h2> -->
                <h3 class="title is-5">Inference results</h3>
                <p>

                </p>
                <div class="box">
                  <iframe src="./static/figures/median_time_survived.html" frameborder="0" width="100%" height="500px"></iframe>
                </div>

                <br>
                <p>
                In the plot we study the median time survived by the prey in the inference environments for
                different configurations. We look at time survived in order to study the relative
                agent performance in each setting. The reason for using median is so that the results are
                not swayed by outliers which might be caused due to no interactions between the predator and
                prey, which might be due to agents getting stuck in an obstacle.
                </p>
                <p>
                To comment further on the nature of learning of the agents trained in different configuration
                and identify the cause of the median time survived values, we look at instances of the
                actual behaviour of these trained agents in the various environments.
                </p>
                <h3 class="title is-6">Normal vision</h3>
                <p>
                  This configuration features agent that are trained on their respective vision style. In the
                  simulation we observe many instances that suggest that both
                  agents have learnt strategies to maximize their reward.
                </p>
                <p>
                  Predator: Chases the prey while keeping
                  it in its depth FOV thus maximizing information and also making up for the lack of memory.
                </p>
                <p>
                  Prey: As seen below, it has visibly learnt ways to evade the predators. Using its monocular vision
                  vision it is able to keep track of the predator and also use obstacles to its advantge
                  by either hiding behind it or going around it. It also uses a zig zag motion to avoid the predator
                  being in its blind spot.
                </p>
            </div>
          </div>
        </div>


        <!--/ Results. -->

      </div>
    </div>
    <!--/ Experiments. -->
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" controls playsinline height="100%">
            <source src="./static/videos/Normal/Trees1.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey evading the predator using the trees to its advantage
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/Normal/hiding.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey hiding behind the rock unseen by the predator
          </h2>
        </br>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" controls playsinline height="100%">
            <source src="./static/videos/Normal/zig-zag.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey exhibitsing zig-zag movement when the predator is behind
          </h2>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" controls playsinline height="100%">
            <source src="./static/videos/Normal/prey-bivsmonovison-behaviour-control.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Different reactions for when the prey is in the binocular and monocular FOV of the predator
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Results. -->
        <!-- Include median_time_survived.html using an iframe -->
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <h3 class="title is-6">Both predator</h3>
            <p>
              We give both the agents predator vison and observe their behaviour. Since the prey is trained using predator vision,
              its blind spot is much larger, thus the prey is unaware if it is being chased from behind.
              It can only rely on cues ahead of it to keep track of objects and the predator.
            </p>
            <p>
              Overall the behviour indicates that the prey has failed to adapt and that further training
              is required.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" controls playsinline height="100%">
            <source src="./static/videos/PredatorBoth/Control.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey failed to learn to effectively evade predator
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/PredatorBoth/trees.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey cannot react to predator approaching from behind (blind spot)
          </h2>
        </br>
        </div>
        <div class="item item-shiba">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/PredatorBoth/not_trained_enough.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey movement indicating that further training is required
          </h2>
        </div>
        <div class="item item-chair-tp">
        </div>
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Results. -->
        <!-- Include median_time_survived.html using an iframe -->
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <h3 class="title is-6">Both prey</h3>
            <p>
              In this configuration both the prey and predator agents are trained on the prey vision,
              which is mostly consisting of monocular vision. As seen in the figure, the median time survived
              is the highest for this configuration.
            </p>
            <p>
              Predator: Since it is trained on prey vision, the predator exhibits less aggressive behaviour and more
              tracking(in order to keep the prey in its narrow depth FOV) by rotating when the prey is in its monocular
              vision. The less aggression exhbited by the predator is a major reason for longer meadian survival time
              of the prey.
            </p>
            <p>
              Prey: We observe that the prey has
              learnt evasive strategies similar to the prey in the normal configuration. This can be
              seen in the video of the prey evading the predator in the control environment.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" controls playsinline height="100%">
            <source src="./static/videos/PreyBoth/Control.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey both performing well in the control environment
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/PreyBoth/rotation.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Rotational behaviour preferred by agent when the adversary is in the monocular FOV
          </h2>
        </br>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" controls playsinline height="100%">
            <source src="./static/videos/PreyBoth/Trees.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey exhibiting evasive maneuvers
          </h2>
        </div>
        <div class="item item-fullbody">
        </div>
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Results. -->
        <!-- Include median_time_survived.html using an iframe -->
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <h3 class="title is-6">Swapped vision</h3>
            <p>
              The aim of this configuration is to comment on the importance of the different vision styles and to test
              whether they are suitable for their respective agents. Swapped vision has lower median values for the
              time survived in all environments except the control environment.
            </p>
            <p>
              Prey: It exhibits behaviours suggesting that it has learnt to keep track of the predator and back up
              to keep running away from the predator. Since it is trained on predator vision, it is unware of the
              objects behind it thus runs into them and even backs up into corners or walls. The prey continues to do so
              until it is eventually caught by the predator as seen in the videos below, possibly due to lack
              of its own positional information and memory.
            </p>
            <p>
              Predator: Since it is trained on prey vision, it continues to rotate and try to keep the prey in its narrow depth
              FOV similar to the both-prey configuration's predator observations.
            </p>
            <p>
              Thus the low median survival times can be explained by the prey's overall lack of
              awareness of objects behind it, causing it to back up into objects, walls and corners making it a sitting duck
              for the predator. In the control environment, this is less of an issue due to lack of objects thus leading
              to a higher median time survived.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" controls playsinline height="100%">
            <source src="./static/videos/Swapped/Trees.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey learns to evade by keep predator in the depth FOV
          </h2>
        </div>

        <div class="item item-shiba">
          <video poster="" id="shiba" controls playsinline height="100%">
            <source src="./static/videos/Swapped/backing-into-corner.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey backs up into a corner trying to evade the predator
          </h2>
        </br>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" controls playsinline height="100%">
            <source src="./static/videos/Swapped/Control-lasts-longer.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey surviving longer in the control environment due to lack of obstacles
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" controls playsinline height="100%">
            <source src="./static/videos/Swapped/backing-into-wall.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Prey backing up into the wall lacking memory and perception behind it
          </h2>
        </div>

          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Conclusions and limitations. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="conclusion">V. Conclusions and Limitations</h2>

        <div class="content has-text-justified">
          <p>
            Our work investigates the role of vision in simulated predator-prey interactions. By varying the vision fields of predator and prey agents and training them using reinforcement learning, we make the following observations:
            <ul>
              <li> Depth information is crucial for predators, while prey also benefit from depth perception to a lesser extent.
              <li> Prey require a wide field of view to navigate and detect predators in their periphery, while predators mainly focus on the direction of their movement.
              <li> Memory complements visual perception in predator-prey interactions.
            </ul>

            These observations support the understanding of why real-world predators prioritize depth perception and prey prioritize a wide field of view in their vision systems.
            </p>
        </div>

        <!-- Limitations. -->
        <h3 class="title is-4">V-A. Limitations and Future Work</h3>
        <div class="content has-text-justified">
          <p>
            Despite interesting observations and promising initial results, our approach has several limitations due to the complex nature of real-life predator-prey interactions. Capturing all these complexities in a simple simulated environment like this is nearly impossible. The main limitations and areas for potential future work include:
          </p>
          <ul>
            <li>
              The predator and prey agents sometimes displayed untrained or random behaviour, suggesting a need for further training. OpenAI's hide and seek environment required approximately 2.7 million episodes for seekers to consistently chase hiders, even with the aid of a curriculum learning approach. Our training lasted only around 10 million steps (approximately \(10^4\) episodes). Although training on OpenAI's scale is not feasible, additional training or alternative strategies like curriculum learning would have likely benefitted the agents greatly.
            </li>
          </ul>

          <!-- TODO: INSERT VIDEO OF RANDOM BEHAVIOUR HERE-->

          <ul>
            <li>
              Memory is essential for complementing vision in predator-prey scenarios, but our agents lacked memory. They forgot objects as soon as they left their field of view and had no positional information for planning trajectories. To address this, we can increase the observation stack to provide more memory and consider including past positional information. Additionally, using a recurrent neural network instead of a multi-layer perceptron could better capture temporal information.
            </li>
          </ul>

          <!-- TODO: INSERT VIDEO OF NO MEMORY HERE -->

          <ul>
            <li>
              Depth perception seems to play an important role for both predators and prey, but our simplified sensor
              assumes no depth perception outside of the binocular region which isn't strictly true. There are various
              monocular depth cues as well (as shown in the figure below) that both predators and prey in reality could
              make use of in their periphery. Thus, adding noisy depth estimates rather than no depth information in the
              monocular regions could help improve learning.
            </li>
          </ul>

          <div class="columns is-centered">
            <div class="column is-half">
              <div align="center" class="box">
                <img src="static/images/monocular-visual-cues.jpg" alt="https://jackwestin.com/resources/mcat-content/perception/perceptual-organization">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Examples of depth cues available from monocular vision</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>


          <ul>
            <li>
              We simplified the action space for our agents by restricting them to backward/forward movement and rotation. In reality, predators and prey have more degrees of freedom, such as the ability to turn their heads and look in different directions while moving. This additional freedom, combined with memory, could provide advantages for prey with predator vision. They could look backward to assess the predator's location and then look forward again to escape, similar to how humans with predator-style vision might behave when being pursued.
            </li>
          </ul>

          <p>
            Furthermore, in this initial work, we only investigate a discrete set of vision fields. However, a similar
            setup could also be used to instead optimise the vision fields for each agent's goal of evading/hunting,
            given some physical constraints.
          </p>

        </div>
        <br/>
        <!--/ Limitations and future work-->

      </div>
    </div>
    <!--/ Conclusions and limitations. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h3 class="title is-3">Become the Predator</h3>
        <div align="center">
          <iframe mozallowfullscreen="true" allow="autoplay; fullscreen"  src="https://play.unity.com/webgl/89a7d2dd-a44b-4efb-af96-1dc4544ddacb" style="border:0px #000000 none;" name="Predator Prey Interaction" scrolling="no" msallowfullscreen="true" allowfullscreen="true" webkitallowfullscreen="true" allowtransparency="true" frameborder="0" marginheight="px" marginwidth="320px" height="540px" width="960px"></iframe>
        </div>
        <h6 class="subtitle has-text-centered"><font size="-0.5"><em>
          Play against a trained prey in our test environment by using your arrow keys. Please note that the predator has been slowed down,
          and you will likely easily be able to catch the prey. It is meant for getting an idea of the setup. The prey behaviour may not
          always be optimal, but if you play a few times you can start to observe some interesting behaviours.</em></font>
        </h6>
        <br>

      </div>
    </div>
  </div>
</section>



<!--section class="section">
  <div class="container is-max-desktop">

    <!--div class="columns is-centered">

      <!-- Visual Effects. -->
      <!--div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!--div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div-->
    <!--/ Matting. -->

<!-- USEFUL SECTION - one main section with subsections -->
    <!-- Animation. -->
    <!--div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <!--h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!--h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      <!--/div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!--div class="columns is-centered">
      <!--div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  <!--/div>
</section-->

<section class="section">
  <div class="container is-max-desktop">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">References</h2>

        <div class="content has-text-justified">
          <ol class="references">
              <li>
                Park J, Lee J, Kim T, Ahn I, Park J. Co-Evolution of Predator-Prey Ecosystems by Reinforcement Learning Agents. Entropy. 2021; 23(4):461. <a href="https://doi.org/10.3390/e23040461">https://doi.org/10.3390/e23040461</a>.
              </li>
              <li>
                Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., & Mordatch, I. (2019). Emergent tool use from multi-agent autocurricula. arXiv preprint <a href="https://arxiv.org/abs/1909.07528">arXiv:1909.07528</a>.
              </li>
              <li>
                Haas, J. K. (2014). A history of the unity game engine.
              </li>
              <li>
                Juliani, A., Berges, V. P., Teng, E., Cohen, A., Harper, J., Elion, C., ... & Lange, D. (2018). Unity: A general platform for intelligent agents. arXiv preprint <a href="https://arxiv.org/abs/1809.02627">arXiv:1809.02627</a>.
              </li>
              <li>
                Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. ArXiv Preprint <a href="https://arxiv.org/abs/1809.02627">ArXiv:1606.01540</a>.
              </li>
              <li>
                Tsai, Dorian, Timothy L. Molloy, and Tristan Perez. "Inverse two-player zero-sum dynamic games." 2016 Australian Control Conference (AuCC). IEEE, 2016.
              </li>
              <li>
                Başar, T., & Olsder, G. J. (1998). Dynamic noncooperative game theory. Society for Industrial and Applied Mathematics.
              </li>
              <li>
                Buşoniu, L., Babuška, R., & De Schutter, B. (2010). Multi-agent reinforcement learning: An overview. Innovations in multi-agent systems and applications-1, 183-221.
              </li>
              <li>
                Shoham, Y., Powers, R., & Grenager, T. (2003). Multi-agent reinforcement learning: a critical survey (Vol. 288). Technical report, Stanford University.
              </li>
              <li>
                Bai, Y., Jin, C., & Yu, T. (2020). Near-optimal reinforcement learning with self-play. Advances in neural information processing systems, 33, 2159-2170.
              </li>
              <li>
                Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., ... & Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419), 1140-1144.
              </li>
              <li>
                Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., ... & Silver, D. (2020). Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839), 604-609.
              </li>
              <li>
                Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009, June). Curriculum learning. In Proceedings of the 26th annual international conference on machine learning (pp. 41-48).
              </li>
              <li>
                Narvekar, S., Peng, B., Leonetti, M., Sinapov, J., Taylor, M. E., & Stone, P. (2020). Curriculum learning for reinforcement learning domains: A framework and survey. The Journal of Machine Learning Research, 21(1), 7382-7431.
              </li>
              <li>
                Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. ArXiv Preprint <a href="https://arxiv.org/abs/1412.6980">arXiv:1412.6980</a></p>
              </li>
              <li>
                Land, M. F., & Nilsson, D. E. (2012). Animal eyes. OUP Oxford.
              </li>
              <!-- Add more references as needed -->
          </ol>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>

<!-- BIBTEXT REFERENCE -->
<!--section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section-->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- LINK TO PAPER -->
      <!--a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a-->
      <a class="icon-link" href="https://github.com/lars-quaedvlieg/Predator-Prey-Unity-MLAgents" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <h6 class="title is-6">Individual Contributions</h6>
          <p>
            L.Q. primarily set up the environment and agents. S.M. primarily implemented the modified ray perception sensor.
            L.Q. and S.M. performed training for different vision configurations.
            A.M. primarily conducted inference and experiments for different configurations and behaviour parameters.
            All three of us wrote the report and worked on the presentation. Overall, we contributed to the project equally.
          </p>
          <p><em>
            This website is based on the <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies website template</a>,
            which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </em></p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
