<!DOCTYPE html>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.3/css/lightbox.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.3/js/lightbox.min.js"></script>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Hunting for Insights: Investigating Predator-Prey Dynamics through
        Simulated Vision and Reinforcement Learning.">
  <meta name="keywords" content="Predator-Prey, Vision, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Hunting for Insights: Investigating Predator-Prey Dynamics through
    Simulated Vision and Reinforcement Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script-->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/Tiger.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://vilab.epfl.ch/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Hunting for Insights: Investigating Predator-Prey Dynamics through
            Simulated Vision and Reinforcement Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://arvind6599.github.io/">Arvind Menon</a>,</span>
            <span class="author-block">
              <a href="https://lars-quaedvlieg.github.io/">Lars C.P.M. Quaedvlieg</a>,</span>
            <span class="author-block">
              <a href="https://vilab.epfl.ch/#prospective">Somesh Mehra</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Swiss Federal Institute of Technology Lausanne (EPFL)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lars-quaedvlieg/Predator-Prey-Unity-MLAgents"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/K0tJrpMla-o"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/arvind6599/PredatorPreyWebsite"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Website</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="box">
      <img src="https://imgtr.ee/images/2023/06/01/S2FLI.png">
      <!--video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%" width="100%" poster="static/videos/loading.gif">
        <source src="static/videos/palmer_trimmed.mp4" type="video/mp4">
      </video-->
    </div>
    <br>
  </div>
</section>

<!-- USEFUL SECTION - static video with a caption -->
<!--section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section-->


<!-- USEFUL SECTION - carousel of videos, can add captions too -->
<!-- section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
            free-viewpoint
            portraits.
          </h2>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section -->




<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This research project investigates how different
            vision fields affect predator-prey interactions. By simulating
            simplified environments and training agents with reinforcement
            learning, we hope to identify trends that emerge in the
            strategies and effectiveness of trained predator and prey agents
            which use varying vision fields. Our methods can help gain
            new insights into the complex interactions between predators
            and prey, whilst improving upon existing literature regarding
            predator-prey simulations.
          </p>
        </div>

      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">

        <h2 class="title is-3">Overview</h2>
        <div class="tile is-ancestor is-centered" style="vertical-align: middle;">
          <div class="tile is-parent">
            <a href="#intro" class="tile is-child box">
              <p class="subtitle"><font size="-1">Introduction</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#related-work" class="tile is-child box">
              <p class="subtitle"><font size="-1">Related Work</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#method" class="tile is-child box">
              <p class="subtitle"><font size="-1">Method</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#exp" class="tile is-child box">
              <p class="subtitle"><font size="-1">Experiments</font></p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#conclusion" class="tile is-child box">
              <p class="subtitle"><font size="-1">Conclusion and Limitations</font></p>
            </a>
          </div>
        </div>

      </div>
    </div>
    <!--/ Abstract. -->


    <!-- USEFUL SECTION - embedded youtube video. -->
    <!--div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div-->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="intro">I. Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Understanding the dynamics between predator-prey in in the natural world is a complex endeavor that
            continues to intrigue researchers. These interactions, characterized by constant adaptation and evolution, offer valuable
            insights into survival strategies employed by animals. Visual intelligence has emerged as a pivotal field of study, providing
            a means to delve deeper into the perceptual and reactive abilities of animals within their environments. In this project, we
            investigate the predator-prey setting, leveraging simplified environments and diverse vision fields to train agents and
            unravel the underlying dynamics.
          </p>
          <p>
            The core challenge that this paper addresses is comprehending the strategies and behaviors exhibited by predators
            and prey in their engagements for different types of vision. To gain valuable insights into the survival tactics employed
            during attacks or hunting endeavors, we employ a simulation of simplified scenarios within controlled environments. In
            addressing this challenge, we adopt a novel approach that involves training agents using a diverse range of vision
            fields, designed to represent distinct types of visions observed in nature. Furthermore, our investigation delves into
            the psychology of chasing, exploring how prey agents can learn to exploit occlusions within their
            environment to their advantage.
          </p>
          <p>
            To evaluate the effectiveness of our approach, we conduct extensive quantitative and qualitative evaluations of the
            agent with different vision fields, which are optimized with reinforcement learning. We examine the emergent behaviors
            exhibited by the trained agents and assess the impact of different vision fields on the success rates of both predators
            and prey.
          </p>
          <p>
            By analyzing the outcomes of these evaluations, we gain a deeper understanding of how variations in visual perception
            can influence the performance and survival capabilities of agents within the predator-prey paradigm.
            (La: Write more on results/conclusions/limitations)
          </p>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Related work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="related-work">II. Related work</h2>
        <div class="content has-text-justified">
          <p>
            The exploration of predator-prey dynamics in the context of vision has received relatively little attention
            in the existing literature. Research on simulated predator-prey systems commonly focuses on population dynamics
            and deals with very simple environments. At the same time, there have been major advancements in multi-agent
            reinforcement learning (MARL), making it possible to discover interesting group dynamics in complex environments.
            Given this, we present below some prior work related to this project.
          </p>
        </div>
      </div>
    </div>
    <!--/ Related work. -->
  </div>
  <br>

  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <!--Predator prey Ecosystems. -->

      <div class="column">
        <div class="content">
          <h2 class="title is-6">Co-Evolution of Predator-Prey Ecosystems by Reinforcement Learning Agents
                                 <a href="https://doi.org/10.3390/e23040461">[1]</a></h2>
          <p>
            This paper explores the use of MARL techniques to simulate the co-evolution mechanisms in
            predator-prey ecosystems. The study demonstrates a biologically plausible approximation of
            the agents' co-evolution over multiple generations in nature.
          </p>
          <div class="box">
            <img src="static/images/predator-prey-behaviour-related-work.png"
                 alt="Comparison of potential distance metric that correlate with reachability.">
            <h5 class="subtitle has-text-centered">
         			<font size="-1.5">
                Left: <span style="font-weight:normal">Initial random location of predators and prey.</span> <br>
                Right: <span style="font-weight:normal">Emergence of swarming among predators and prey.</span>
         			</font>
       			</h5>
          </div>
          <p>
            This work however is limited to a simplified 2D environment without incorporating vision.
          </p>
        </div>
      </div>
      <!--/ Predator prey Ecosystems.-->

      <!-- OpenAI hide and seek. -->
      <div class="column">
        <h2 class="title is-6">Emergent tool use from multi-agent autocurricula
                               <a href="https://openai.com/research/emergent-tool-use">[2]</a></h2>
        <p>
          Research conducted by OpenAI investigates the emergence of sophisticated tool use and coordination among agents in
          a hide-and-seek game environment, which shares similarities with predator-prey interactions. The study showcases the
          potential of multi-agent self-play in generating emergent auto curricula. Notably, this work trains agents in a
          complex environment equipped with vision sensors.

        </p>
        <div class="box">
          <img src="static/images/openai-hide-and-seek.gif"
               alt="OpenAI Hide and Seek">
          <h5 class="subtitle has-text-centered">
            <font size="-1.5">
              <span style="font-weight:normal">Use of vision sensors in the hide-and-seek environment</span>
            </font>
          </h5>
        </div>
         <p>
           The study however does not explicitly explore the effects of varying the vision fields on the emergent behaviors of the agents.
         </p>
      </div>
    </div>
      <!--/ OpenAI hide and seek. -->
  </div>
  <br>

  <div class="container is-max-desktop">
    <!-- Related work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Given the existing gaps in the literature for the study of predator-prey dynamics in the context of vision, our work aims
            to address them by incorporating various vision fields to represent different predators and prey. Through this approach,
            we investigate the impact of diverse visual perspectives on the emergent behaviors of the agents, providing valuable insights
            into the adaptive strategies employed in predator-prey interactions.
          </p>
        </div>
      </div>
    </div>
    <!--/ Related work. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="method">III. Method</h2>
        <div class="content has-text-justified">
          <p>
            In this section, we present the methodology utilized for this paper. We first discuss the environment setup,
            after which we detail how the predator and prey agents are created. Finally, we talk about how the agents are optimized.
          </p>
          <p>
            We create a 3D environment using the Unity game development engine <a href="https://unity.com/">[3]</a>. The selection of Unity as the engine
            for creating the environment instead of a different simulator is motivated by several key factors. Mainly, Unity's ML-Agents
            package <a href="https://arxiv.org/abs/1809.02627">[4]</a> offers support for the OpenAI Gym framework
            <a href="https://arxiv.org/abs/1809.02627">[5]</a>, which is widely used in
            reinforcement learning research. This way, Unity enables interaction between the environment and the learning algorithms,
            allowing for efficient experimentation and evaluation of predator-prey agents.
          </p>
          <p>
            Moreover, the engine offers a wide range of built-in tools, such as physics simulations, ray tracing, and collision.
            Finally, Unity provides a user-friendly and intuitive development environment, making it accessible to researchers with
             varying levels of expertise.
          </p>
        </div>
        <!-- Creating environments. -->
        <h3 class="title is-4">III-I. Environments</h3>
        <div class="content has-text-justified">
          <p>
            We present a diverse set of simulated environments aimed at replicating various scenarios involving predator-prey interactions.
            Our objective is to foster a wide range of hunting behaviors for predators while prey organisms strive to survive. To achieve
            this diversity, we incorporate obstacles such as trees and rocks within the environments. Additionally, we introduce walls along
            the boundaries to confine the agents to the designated training area. This confinement allows us to concentrate the agents' actions
             and observations within a controlled environment.
          </p>
          <p>
            Furthermore, we integrate physics-based simulations and collision detection mechanisms into the environment.
            By assigning colliders to the predator and prey agent models, we ensure accurate detection of interactions with the
            environment and other agents. Each object in the environment is assigned a corresponding tag, such as "obstacle",
            "predator", or "prey", enabling efficient identification.
          </p>
          <p>
            To speed up the learning process, we design the initial training environment ('Control' shown below) to be free of
            obstacles. This approach eliminates potential obstructions that could slow down training progress in the early stages.
            Subsequently, we create three additional training environments ('Forest', 'Escape Room' and 'Rocky') featuring distinct obstacle types:
            small trees, large rocks, and a split doorway. These environments allow the agents to acquire the skills necessary for
            navigating through increasingly complex situations involving obstacles, and should allow the prey agent to learn to exploit
             occlusions.
          </p>
        </div>
        <!--/ Creating environments. -->

      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <a href="static/images/training/control_train.jpg" data-lightbox="image">
            <img src="static/images/training/control_train.jpg" alt="Image 1">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Control</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment 1</p>
          </div>
        </div>
        <div class="item">
          <a href="static/images/training/forrest.jpg" data-lightbox="image">
            <img src="static/images/training/forrest.jpg" alt="Image 2">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Forest</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment 2</p>
          </div>
        </div>
        <div class="item">
          <a href="static/images/training/holes.jpg" data-lightbox="image">
            <img src="static/images/training/holes.jpg" alt="Image 3">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Escape Room</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment 3</p>
          </div>
        </div>
        <div class="item">
          <a href="static/images/training/rocky.jpg" data-lightbox="image">
            <img src="static/images/training/rocky.jpg" alt="Image 4">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Rocky</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment 4</p>
          </div>
        </div>
      </div>
    </div>
    <h6 class="subtitle has-text-centered">
      Environments used for training the agents
    </h6>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- Creating agents. -->
        <h3 class="title is-4">III-II. Predator and Prey Agents</h3>
        <div class="content has-text-justified">
          <p>
            Whilst the prey and predator have opposing objectives, they demonstrate similar characteristics. Both agents operate
            within a continuous action space with two degrees of freedom: rotation about the \(y\)-axis and forward/backward movement.
            Each degree of freedom allows the agent to select a value between \([-1, 1]\), determining the direction and intensity of
             movement. The agents' behavioral parameters consist of maximum movement and rotation speeds, which scale this intensity
              value. Notably, the predator possesses an additional box collider located at its face, resulting in the prey's death
              upon collision. Conversely, the prey lacks this collider as its primary goal involves evading the predator.
          </p>
          <br>
          <div class="columns is-centered">
            <div class="column is-three-fifths">
              <div align="center" class="box">
                <img src="static/images/agent-box-colliders.jpeg" alt="Box colliders for the agents">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Box colliders for the predator and prey agents</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>

          <br>
        </div>
        <!--/ Creating agents. -->

        <!-- Vision sensor. -->
        <h3 class="title is-5">Vision sensor</h3>
        <div class="content has-text-justified">
          <p>
            In order to simulate various vision fields, we employ a modified version of the ML-Agents Ray Perception sensor. This
            sensor uses a collection of ray casts originating from a central point. Several parameters are available to manipulate
             the vision, including ray angles (to establish the field of view), ray length (to determine the depth of field), number
             of rays (to regulate ray density), and a list of tags for filtering detected objects. Each ray conveys the following
             information: a flag indicating whether it collides with a tagged object, a one-hot encoded vector identifying the object
              type, and the normalized distance to the hit object relative to the ray length. Consequently, the sensor's output
              comprises a flattened vector encapsulating the information encoded within each ray.
          </p>
          <p>
            To control the region of binocular vision, we have extended this sensor to incorporate a parameter representing
            the number of depth rays. We make the simple assumption that agents can see the presence and type
            of an object across their entire field of view, but only have depth perception in their binocular region.
            Consequently, we exclude depth information for any rays falling outside this region by
            assigning a distance of \(-1\). Using this modified Ray Perception sensor, we define two distinct sensor types:
            <em>predator-style</em> and <em>prey-style</em>, which draw inspiration from the characteristic vision traits observed in real-world
            predators and prey <a href="https://books.google.ch/books?id=uXSK6hDKFC0C&oi=fnd&pg=PP1&dq=animal+eyes+&ots=dLMn7ufvNI&sig=Aedtf7xH87s3c9gMFKzPWSvj1Ms&redir_esc=y#v=onepage&q=animal%20eyes&f=false">[16]</a>, respectively.
          </p>
          <br>
          <div align="center" class="box">
            <img src="static/images/ray-perception-sensors.png" alt="Predator and prey-style ray perception sensors">
            <h5 class="subtitle has-text-centered">
              <font size="-0.7">
                <span style="font-weight:normal">Predator- and prey-style ray perception sensors</span>
              </font>
            </h5>
          </div>

          <br>
          <p>
            Utilizing the modified Ray Perception sensor offers distinct advantages for this paper by providing precise
             control over the simulated vision fields. Customizable parameters such as field of view and depth of field enable
             alignment with the specific requirements of the prey and predator agents in the simulated environment. This level of
             control ensures that the sensory input accurately reflects the desired characteristics of the vision of the agent.
             Furthermore, using the modified Ray Perception sensor instead of raw camera inputs enhances training efficiency by
             reducing the computational burden of processing high-dimensional image data.
          </p>
          <br>
        </div>
        <!--/ Vision sensor. -->

        <!-- Training. -->
        <h3 class="title is-4">III-III. Setting up Reinforcement Learning Training</h3>
        <div class="content has-text-justified">
          <p>
            The setting of predator-prey dynamics can be framed as a two-player dynamic game <a href="https://ieeexplore.ieee.org/abstract/document/7868186">[6]</a>,
            <a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611971132.bm">[7]</a>. This game-theoretic
            formulation captures the interactions and influence of the agents on the state of the game, providing a framework
            for studying predator-prey dynamics based on a common utility function. The game is only played for a set number of
            steps, until it is terminated.
          </p>

          <p>
            In our formulation, the state and action spaces align with the descriptions provided in the previous sections,
            allowing for a consistent representation of the game dynamics. The transitions between states are determined by
            the underlying physics engine, ensuring a realistic simulation of the predator-prey environment.
          </p>

          <p>
            $$
              G_\text{pred} = \sum_{i=1}^{T}\left[\frac{\mathbb{1}(\text{prey caught at } t)}{N_\text{prey}} - \frac{1}{T}\right]
            $$
            $$
              G_\text{prey} = - G_\text{pred}
            $$
          </p>

          <p>
            The reward structure employed in our proposed framework, as illustrated in the figure above, incorporates a constant time
            penalty applied to the predator at each time step, and a positive reward if a prey is caught. This deliberate design
            choice serves to incentivize the predator to capture prey at the earliest opportunity. Conversely, the reward for the
            prey is formulated in the opposite manner, establishing a zero-sum game. The range of rewards is bounded within the
            interval \([-1, 1]\). This reward specification enables a natural distinction between favorable and unfavorable
            outcomes of both agents.
          </p>

          <p>
            Furthermore, since the predator-prey game exhibits an inherent asymmetry, it can be expressed as an asymmetric
            zero-sum game. Unlike games such as soccer, where teams have shared objectives, the predator and prey agents
            pursue conflicting goals. The asymmetry arises from the distinct policies used by the different agents, resulting
            in strategic dynamics that differ from those observed in symmetric games.
          </p>

          <p>
            Overall, the nature of the game poses additional challenges, as it suffers from many of the problems in
            competitive multi-agent learning <a href="https://link.springer.com/chapter/10.1007/978-3-642-14435-6_7">[8]</a>. Multi-agent reinforcement learning in competitive scenarios
            poses several challenges <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=46662ca0627974ec3c3c038ccc78b2ef9ef92218">[9]</a>. This paper employs a prominent technique called "self-play"
            <a href="https://proceedings.neurips.cc/paper/2020/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf">[10]</a>, <a href="https://www.science.org/doi/pdf/10.1126/science.aar6404?casa_token=U6oPNZ67XRcAAAAA%3Ai-nCPopyruTropl7_QoP3RWxM93tlHd75lV826n7-8X5_XlX0EniJPBxQ9KnsNYUHSxFyeLGBa8juko&">[11]</a>
            to address these difficulties. The diagram below provides a high-level overview of this approach.
          </p>
          <br>
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <div align="center" class="box">
                <img src="static/images/self-play.jpeg" alt="Self play">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Diagram showing the self-play mechanism</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>



          <br>

          <p>
            Initially, either the predator or prey agent is trained, while the model of the other agent remains frozen. After
            a predetermined number of iterations, the frozen model is trained, and the other agent's model is frozen.
            However, a potential issue arises from the bias introduced by repeatedly playing against the most recent model.
            This bias can lead to overfitting and poor generalization. To mitigate this problem, we incorporate an ELO ranking
            system as done in MuZero <a href="https://www.nature.com/articles/s41586-020-03051-4">[12]</a>, enabling models to compete against earlier versions of their opponents.
            By doing so, we reduce the impact of bias, leading to improved generalization and robustness in the learned policies.
          </p>
          <br>

        </div>
        <!--/ Training. -->

      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="exp">IV. Experiments</h2>

        <p>
          In this section, we discuss the setup and results of the experiments in detail. Regarding results, we first discuss the training progression, after which we do both a qualitative and
          a quantitative analysis of the different agent configurations.
        </p>
        <br>

        <!-- Inference environments. -->
        <h3 class="title is-4">IV-I. Experimental Setup</h3>
        <br/>
        <div class="content has-text-justified">
          <div class="container">
            <!-- <h2 class="title is-2 has-text-centered">Model and Behavior Parameters</h2> -->
                <h3 class="title is-5">Agent parameters</h3>
                <p>
                  \lars{We should also discuss the specific stuff like movement speed here}
                  Behavioural and vision params. rotation and movement speeds and specific params of sensors.
                </p>

                <div class="columns is-centered">
                  <div class="column is-two-fifths">
                    <div class="box">
                      <img src="static/images/configs_table.png" alt="Vision sensor configurations">
                      <h5 class="subtitle has-text-centered">
                        <font size="-0.7">
                          <span style="font-weight:normal">
                            The combinations of vision sensors used for the agents in the experiments.
                        </span>
                        </font>
                      </h5>
                    </div>
                  </div>
                </div>


          </div>
        </div>

        <div class="box">
          <div class="table-container">
              <table class="table" align="center">
                  <thead>
                      <tr>
                          <th></th>
                          <th>Rays per direction</th>
                          <th>Depth rays per direction</th>
                          <th>Max ray degrees</th>
                          <th>Ray length</th>
                          <th>Observation stacks</th>
                          <th>Field of view</th>
                          <th>Binocular region</th>
                      </tr>
                  </thead>
                  <tbody>
                      <tr>
                          <td><b>Prey-style</b></td>
                          <td>30</td>
                          <td>4</td>
                          <td>160</td>
                          <td>15</td>
                          <td>5</td>
                          <td><em>320°</em></td>
                          <td><em>43°</em></td>
                      </tr>
                      <tr>
                          <td><b>Predator-style</b></td>
                          <td>30</td>
                          <td>20</td>
                          <td>85</td>
                          <td>15</td>
                          <td>5</td>
                          <td><em>170°</em></td>
                          <td><em>113°</em></td>
                      </tr>
                  </tbody>
              </table>
          </div>
          <div align="center">
          <font size="-0.7">
            <span style="font-weight:normal">
              Parameters of the prey and predator-style vision sensors
          </span>
          </font>
          </div>
        </div>


        <div class="content has-text-justified">
          <div class="container">

            <h3 class="title is-5">Training details</h3>
            <p>
              The agents are trained on all environments mentioned in the methodology section in parallel. By having environments of different difficulties,
              a speed-up in training can be observed, similar as in curriculum learning <a href="https://dl.acm.org/doi/abs/10.1145/1553374.1553380?casa_token=nh2ZrojaWoAAAAAA:ch-UtBUm-jXdt36EJoGrONsdXTfKYTOF9g3wpMqUjykIzeeCv2_d4p2hoGlyuJRGTlWCdZyWmPVmmg">[13]</a>,
              <a href="https://dl.acm.org/doi/abs/10.5555/3455716.3455897">[14]</a>. Furthermore, three agents of each type were present
              in each environment. Although this could make the environment unstable due to partial observability, it greatly enhances training speed.
            </p>
            <p>
              The predator and prey were each trained self-play for \(9 \cdot 10^6\) steps, where one step corresponds to
              one frame in the simulation. The swap between the frozen training teams happens every \(3 \cdot 10^5\) training steps. At
              each episode, the agent is spawned with a random rotation the y-axis, and the maximum episode duration is \(10^4\).
              The model will play against the most recent version of the agent \(80\%\) of the time. The models both consist of
              a \(3\) hidden-layer MLP with \(512\) neurons each with batch at each layer. The model is optimized using
              PPO with the Adam <a href="https://arxiv.org/abs/1412.6980">[15]</a> optimizer with a learning of \(0.0003\) and no decay. Finally, the buffer sizes of both
              agents are set to \(40960\).
            </p>

            <h3 class="title is-5">Inference environments</h3>
            <p>
              For testing the agents we design new out of distribution environments with varying levels of complexity. The control
              environment, like in training, consists of an empty environment. Additional environments include similar obstacles and
              hiding spots in different configurations, introducing more blind spots for the prey to utilize for protection. We also
              incorporate a complex "hide and seek" environment that encourages the agents to showcase
              interesting behaviors and interactions. The inference environments are shown below.
            </p>

          </div>
        </div>

        <br/>


      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <a href="./static/images/inference/control.jpg" data-lightbox="image">
            <img src="./static/images/inference/control.jpg" alt="Control Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Control</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Control</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/hide.jpg" data-lightbox="image">
            <img src="./static/images/inference/hide.jpg" alt="Hide Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Hide</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Hide</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/seek.jpg" data-lightbox="image">
            <img src="./static/images/inference/seek.jpg" alt="Seek Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Seek</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Seek</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/rocks.jpg" data-lightbox="image">
            <img src="./static/images/inference/rocks.jpg" alt="Rocks Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Rocks</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Rocks</p>
          </div>
        </div>
        <div class="item">
          <a href="./static/images/inference/trees.jpg" data-lightbox="image">
            <img src="./static/images/inference/trees.jpg" alt="Trees Image">
          </a>
          <div class="caption">
            <h2 class="subtitle has-text-centered"><font size="-0.7">Trees</font></h2>
            <p class="subcaption has-text-centered">Screenshot of Training Environment: Trees</p>
          </div>
        </div>
      </div>
    </div>
    <h6 class="subtitle has-text-centered">
      Environments used for inference
    </h6>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <div class="content has-text-justified">
          <div class="container">
            <h3 class="title is-5">Hardware</h3>
            <p>
              The models were trained using a CUDA-enabled NVIDIA
              V100 PCIe 32 GB GPU with 7TFLOPS, a Xeon-Gold processor running with a 2.1 GHz clock speed, and 16GB of
              RAM. Our method is implemented with Unity’s ML-Agents package and PyTorch, running on Linux with Python 3.7.
            </p>

          </div>
        </div>

      </div>
    </div>
    <!--/ Experiments. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Results. -->
        <!-- Include median_time_survived.html using an iframe -->
        <h3 class="title is-4">IV-II.Results</h3>
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <div class="container">
              <!-- <h2 class="title is-2 has-text-centered">Model and Behavior Parameters</h2> -->
                  <h3 class="title is-5">Training results</h3>
                  <div class="box">
                      <img src="./static/images/training-plot.png" alt="Reward plot during training">
                      <h5 class="subtitle has-text-centered">
                        <font size="-0.7">
                          <span style="font-weight:normal">
                            (Left) Average returns over \(9\) million steps of training for each agent configuration.
                            (Right) Average episode lengths over \(9\) million steps of training for each agent configuration.
                            Note that, unfortunately, the training logs for 'Normal' vision were lost. However the trend in rewards
                            was similar to the other plots shown here. Furthermore, take note that the x-axis represents the number of steps,
                            so if the predator started training, it would swap at the step \(300000\), and the x-label for the prey at total step \(300005\)
                            would be \(5\). An exponential smoothing of strength \(0.8\) was used.
                        </span>
                        </font>
                      </h5>
                  </div>
                  <p>
                    The average returns during training for various vision configurations of predator and prey agents are illustrated in the left figure. It is
                    observed that the PredatorBoth vision exhibits a slower learning rate compared to other configurations, likely due to the reduced field of
                    view when both agents have predator vision. However, this effect diminishes over time. Conversely, the SwappedVision configuration stands out
                     as advantageous for the predators, attributed to the vision setup that grants predators a wider field of view while restricting the prey's vision.
                     Despite the impressive hunting prowess of predators, prey agents still demonstrate substantial learning progress. This finding highlights the
                     adaptability and refinement of evasive maneuvers by prey agents, even in the presence of highly skilled predators.
                  </p>
                  <p>
                    In the right figure, we analyze the episode length throughout the training process. Interestingly, we consistently observe a decreasing trend in episode
                    length across all configurations. This reduction reflects a notable enhancement in the predator's decision-making capabilities and their aptitude for
                    achieving goals within shorter timeframes.
                  </p>
                  <br>
            </div>


            <div class="container">
              <!-- <h2 class="title is-2 has-text-centered">Model and Behavior Parameters</h2> -->
                  <h3 class="title is-5">Inference results</h3>
                  <p>
                    Inference results
                  </p>
                  <div class="box">
                    <iframe src="./static/figures/median_time_survived.html" frameborder="0" width="100%" height="500px"></iframe>
                  </div>
            </div>
          </div>
        </div>


        <!--/ Results. -->

      </div>
    </div>
    <!--/ Experiments. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Conclusions and limitations. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="conclusion">V. Conclusions and Limitations</h2>

        <div class="content has-text-justified">
          <p>With this work, we have developed a new way to investigate the role of vision in simulated predator-prey
            interactions. Through our experiments where we vary the vision fields of predator and prey agents and train
            them against each other using reinforcement learning, we are able to make the following overarching observations:</p>
          <ul>
              <li>Depth information seems to be crucial for predators. Even for prey, depth can act as an important cue but
                to a lesser extent.</li>
              <li>Wide field of view is crucial to prey. Whereas predators mainly need to look in the direction in which they
                are moving at most times, prey needs both to see where they're going while also being able to sense predators in
                their periphery to evade them.</li>
              <li>Memory plays an important role in complementing visual perception in predator-prey interactions.</li>
          </ul>
          <p>These observations support our current understanding of why real-world predators and prey developed vision systems
            that prioritized depth perception and field of view, respectively.</p>

        </div>

        <!-- Limitations. -->
        <h3 class="title is-4">V-I. Limitations and Future Work</h3>
        <div class="content has-text-justified">
          <p>
            Whilst we observe many interesting behaviors and were able to obtain promising initial results, it's important
            to note there are many limitations to our approach. Real-life predator prey interactions are incredibly complex,
            and it's almost impossible to capture all these complexities in a simple simulated environment such as this. The most
            pertinent limitations and potential future work are as follows:
          </p>
          <ul>
            <li>
              The predator and prey agents exhibited seemingly untrained or random behaviour at times, indicating that the agents
              needed to be trained further. For comparison, in OpenAI's hide and seek environment it took ~2.7 million <em>episodes</em> for
              the seekers to learn to consistently chase the hiders, and they even used a curriculum learning approach to help facilitate
              learning. Meanwhile our training ran for only ~10 million steps in total (~corresponds to an order of magnitude \(10^4\)
              episodes). Whilst training at OpenAI's scale is of course not feasible, the agents would likely have benefited greatly
              from further training or an alternative strategy such as curriculum learning.
            </li>
          </ul>

          <!-- TODO: INSERT VIDEO OF RANDOM BEHAVIOUR HERE-->

          <ul>
            <li>
              As we observed, memory is crucial to complement vision in predator-prey scenarios, however our agents
              had virtually no memory. Not only would the agents forget about an object as soon as it fell out of its
              field of view, but since they didn't have any positional information they also couldn't plan a smarter
              trajectory knowing where they'd been and what they'd seen (e.g. to avoid backing into a wall and not going
              anywhere). To address this, we could increase the observation stack to give the agents more memory, and
              potentially include past positional information as an additional observation. We could also try experimenting
              with a recurrent neural network instead of a vanilla multi-layer perceptron to better model the temporal information.
            </li>
          </ul>

          <!-- TODO: INSERT VIDEO OF NO MEMORY HERE -->

          <ul>
            <li>
              Depth perception seems to play an important role for both predators and prey, but our simplified sensor
              assumes no depth perception outside of the binocular region which isn't strictly true. There are various
              monocular depth cues as well (as shown in the figure below) that both predators and prey in reality could
              make use of in their periphery. Thus, adding noisy depth estimates rather than no depth information in the
              monocular regions could help improve learning.
            </li>
          </ul>

          <div class="columns is-centered">
            <div class="column is-half">
              <div align="center" class="box">
                <img src="static/images/monocular-visual-cues.jpg" alt="https://jackwestin.com/resources/mcat-content/perception/perceptual-organization">
                <h5 class="subtitle has-text-centered">
                  <font size="-0.7">
                    <span style="font-weight:normal">Examples of depth cues available from monocular vision</span>
                  </font>
                </h5>
              </div>
            </div>
          </div>


          <ul>
            <li>
              Another major simplification we made is in terms of the action space of the agents. Our agents can only move
              backward/forwards or rotate, however, there are many more degrees of freedom for real-life predators and prey,
              some of which could prove very advantageous in such scenarios. For example, in reality, it would be possible for
              an agent to turn its head to see in a different direction from where it's moving. Having this additional degree of
              freedom coupled with memory could allow the prey with predator vision to look backward, gauge where the predator is,
              and then look forward again to run away — similar to how we as humans (who have predator-style vision) might behave
              when being chased.
            </li>
          </ul>

          <p>
            Furthermore, in this initial work, we only investigate a discrete set of vision fields, however a similar
            setup could also be used to instead optimise the vision fields for each agent's goal of evading/hunting
            given some physical constraints.
          </p>

        </div>
        <br/>
        <!--/ Limitations and future work-->

      </div>
    </div>
    <!--/ Conclusions and limitations. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h3 class="title is-3">Become the Predator</h3>
        <div align="center">
          <iframe mozallowfullscreen="true" allow="autoplay; fullscreen"  src="https://play.unity.com/webgl/89a7d2dd-a44b-4efb-af96-1dc4544ddacb" style="border:0px #000000 none;" name="Predator Prey Interaction" scrolling="no" msallowfullscreen="true" allowfullscreen="true" webkitallowfullscreen="true" allowtransparency="true" frameborder="0" marginheight="px" marginwidth="320px" height="540px" width="960px"></iframe>
        </div>
        <h6 class="subtitle has-text-centered"><font size="-0.5"><em>
          Play against a trained prey in our test environment by using your arrow keys. Please note that the predator has been slowed down,
          and you will likely easily be able to catch the prey. It is meant for getting an idea of the setup. The prey behaviour may not
          always be optimal, but if you play a few times you can start to observe some interesting behaviours.</em></font>
        </h6>
        <br>

      </div>
    </div>
  </div>
</section>



<!--section class="section">
  <div class="container is-max-desktop">

    <!--div class="columns is-centered">

      <!-- Visual Effects. -->
      <!--div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!--div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div-->
    <!--/ Matting. -->

<!-- USEFUL SECTION - one main section with subsections -->
    <!-- Animation. -->
    <!--div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <!--h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!--h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      <!--/div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!--div class="columns is-centered">
      <!--div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  <!--/div>
</section-->

<section class="section">
  <div class="container is-max-desktop">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">References</h2>

        <div class="content has-text-justified">
          <ol class="references">
              <li>
                Park J, Lee J, Kim T, Ahn I, Park J. Co-Evolution of Predator-Prey Ecosystems by Reinforcement Learning Agents. Entropy. 2021; 23(4):461. <a href="https://doi.org/10.3390/e23040461">https://doi.org/10.3390/e23040461</a>.
              </li>
              <li>
                Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., & Mordatch, I. (2019). Emergent tool use from multi-agent autocurricula. arXiv preprint <a href="https://arxiv.org/abs/1909.07528">arXiv:1909.07528</a>.
              </li>
              <li>
                Haas, J. K. (2014). A history of the unity game engine.
              </li>
              <li>
                Juliani, A., Berges, V. P., Teng, E., Cohen, A., Harper, J., Elion, C., ... & Lange, D. (2018). Unity: A general platform for intelligent agents. arXiv preprint <a href="https://arxiv.org/abs/1809.02627">arXiv:1809.02627</a>.
              </li>
              <li>
                Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. ArXiv Preprint <a href="https://arxiv.org/abs/1809.02627">ArXiv:1606.01540</a>.
              </li>
              <li>
                Tsai, Dorian, Timothy L. Molloy, and Tristan Perez. "Inverse two-player zero-sum dynamic games." 2016 Australian Control Conference (AuCC). IEEE, 2016.
              </li>
              <li>
                Başar, T., & Olsder, G. J. (1998). Dynamic noncooperative game theory. Society for Industrial and Applied Mathematics.
              </li>
              <li>
                Buşoniu, L., Babuška, R., & De Schutter, B. (2010). Multi-agent reinforcement learning: An overview. Innovations in multi-agent systems and applications-1, 183-221.
              </li>
              <li>
                Shoham, Y., Powers, R., & Grenager, T. (2003). Multi-agent reinforcement learning: a critical survey (Vol. 288). Technical report, Stanford University.
              </li>
              <li>
                Bai, Y., Jin, C., & Yu, T. (2020). Near-optimal reinforcement learning with self-play. Advances in neural information processing systems, 33, 2159-2170.
              </li>
              <li>
                Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., ... & Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419), 1140-1144.
              </li>
              <li>
                Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., ... & Silver, D. (2020). Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839), 604-609.
              </li>
              <li>
                Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009, June). Curriculum learning. In Proceedings of the 26th annual international conference on machine learning (pp. 41-48).
              </li>
              <li>
                Narvekar, S., Peng, B., Leonetti, M., Sinapov, J., Taylor, M. E., & Stone, P. (2020). Curriculum learning for reinforcement learning domains: A framework and survey. The Journal of Machine Learning Research, 21(1), 7382-7431.
              </li>
              <li>
                Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. ArXiv Preprint <a href="https://arxiv.org/abs/1412.6980">arXiv:1412.6980</a></p>
              </li>
              <li>
                Land, M. F., & Nilsson, D. E. (2012). Animal eyes. OUP Oxford.
              </li>
              <!-- Add more references as needed -->
          </ol>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>

<!-- BIBTEXT REFERENCE -->
<!--section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section-->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- LINK TO PAPER -->
      <!--a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a-->
      <a class="icon-link" href="https://github.com/lars-quaedvlieg/Predator-Prey-Unity-MLAgents" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <h6 class="title is-6">Individual Contributions</h6>
          <p>
            Individual contributions here
          </p>
          <p><em>
            This website is based on the <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies website template</a>,
            which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </em></p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
